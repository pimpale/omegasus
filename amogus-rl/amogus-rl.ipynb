{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 09:48:06.266643: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-29 09:48:07.202448: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "from dataclasses import dataclass\n",
    "import concurrent.futures\n",
    "from collections import defaultdict\n",
    "import typing\n",
    "from torch import optim\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import copy\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import env\n",
    "import network\n",
    "import player\n",
    "\n",
    "\n",
    "BOARD_XSIZE = env.BOARD_XSIZE\n",
    "BOARD_YSIZE = env.BOARD_YSIZE\n",
    "\n",
    "DIMS=(BOARD_XSIZE,BOARD_YSIZE)\n",
    "\n",
    "\n",
    "EPISODES_PER_AGENT = 128\n",
    "TRAIN_EPOCHS = 500000\n",
    "MODEL_SAVE_INTERVAL = 100\n",
    "MAKE_OPPONENT_INTERVAL = 2000\n",
    "SUMMARY_STATS_INTERVAL = 10\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "SUMMARY_DIR = './summary'\n",
    "MODEL_DIR = './models'\n",
    "\n",
    "# create result directory\n",
    "if not os.path.exists(SUMMARY_DIR):\n",
    "    os.makedirs(SUMMARY_DIR)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "cuda = torch.device(\"cuda\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "    device = cuda\n",
    "else:\n",
    "    device = cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "impostor_actor = network.Actor().to(device)\n",
    "impostor_critic = network.Critic().to(device)\n",
    "impostor_actor_optimizer = optim.Adam(impostor_actor.parameters(), lr=network.ACTOR_LR)\n",
    "impostor_critic_optimizer = optim.Adam(impostor_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "crewmate_actor = network.Actor().to(device)\n",
    "crewmate_critic = network.Critic().to(device)\n",
    "crewmate_actor_optimizer = optim.Adam(crewmate_actor.parameters(), lr=network.ACTOR_LR)\n",
    "crewmate_critic_optimizer = optim.Adam(crewmate_critic.parameters(), lr=network.CRITIC_LR)\n",
    "\n",
    "# Get Writer\n",
    "writer = SummaryWriter(log_dir=SUMMARY_DIR)\n",
    "\n",
    "impostor_step = 0\n",
    "crewmate_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_valid_location() -> tuple[int, int]:\n",
    "    x = np.random.randint(0, BOARD_XSIZE)\n",
    "    y = np.random.randint(0, BOARD_YSIZE)\n",
    "    return (x, y)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GameSummary:\n",
    "    configuration: tuple[str, str, str]\n",
    "    actor_is_impostor: bool\n",
    "\n",
    "\n",
    "def play(\n",
    "    actor_engine: player.ActorPlayer,\n",
    "    actor_is_impostor: bool,\n",
    "    other_engines: list[player.Player],\n",
    ") -> tuple[\n",
    "    list[env.Observation],\n",
    "    list[env.Action],\n",
    "    list[float],\n",
    "    list[float],\n",
    "    list[float],\n",
    "    GameSummary,\n",
    "]:\n",
    "    # create environment\n",
    "    initial_state = env.State(\n",
    "        {},\n",
    "        np.zeros((BOARD_XSIZE, BOARD_YSIZE), dtype=np.int8),\n",
    "        np.zeros((BOARD_XSIZE, BOARD_YSIZE), dtype=np.int8),\n",
    "    )\n",
    "\n",
    "    # randomize task location\n",
    "    for _ in range(10):\n",
    "        location = random_valid_location()\n",
    "        initial_state.tasks[location] += 3\n",
    "\n",
    "    # create actor player at random location\n",
    "    actor_state = env.PlayerState(random_valid_location(), actor_is_impostor)\n",
    "    # create other players at random locations\n",
    "    other_state = [\n",
    "        env.PlayerState(random_valid_location(), False) for _ in other_engines\n",
    "    ]\n",
    "\n",
    "    # set the players in the environment\n",
    "    initial_state.players = {str(i): s for i, s in enumerate(other_state)}\n",
    "    initial_state.players[\"actor\"] = actor_state\n",
    "\n",
    "    # set the player data\n",
    "    agent_engines = {str(i): e for i, e in enumerate(other_engines)}\n",
    "    agent_engines[\"actor\"] = actor_engine\n",
    "\n",
    "    impostor = (\n",
    "        str(np.random.randint(0, len(other_engines))) if actor_is_impostor else \"actor\"\n",
    "    )\n",
    "\n",
    "    e = env.AmogusEnv(initial_state)\n",
    "\n",
    "    s_t: list[env.Observation] = []\n",
    "    a_t: list[env.Action] = []\n",
    "    r_t: list[float] = []\n",
    "    # play the game\n",
    "    last_obs = e.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # gather actions\n",
    "        actions = {}\n",
    "        for agent, agent_engine in agent_engines.items():\n",
    "            chosen_action = agent_engine.play(agent == impostor, last_obs[agent])\n",
    "            actions[agent] = chosen_action\n",
    "            # if the player is the actor we're gathering data for, then we need to store the data\n",
    "            if agent == \"actor\":\n",
    "                s_t += [last_obs[agent]]\n",
    "                a_t += [chosen_action]\n",
    "        \n",
    "        # step\n",
    "        last_obs, rewards, terminateds, truncateds, _ = e.step(actions)\n",
    "\n",
    "        # add rewards\n",
    "        r_t += [rewards[\"actor\"]]\n",
    "\n",
    "        for agent in last_obs.keys():\n",
    "            if terminateds[agent] or truncateds[agent]:\n",
    "                del agent_engines[agent]\n",
    "                # if the actor we're gathering data for is dead, then we need to stop\n",
    "                if agent == \"actor\":\n",
    "                    done = True\n",
    "\n",
    "    # compute advantage and value\n",
    "    critic_network = impostor_critic if actor_is_impostor else crewmate_critic\n",
    "    d_t = network.compute_advantage(critic_network, s_t, r_t)\n",
    "    v_t = network.compute_value(r_t)\n",
    "\n",
    "    summary = GameSummary(\n",
    "        tuple(sorted(e.name() for e in other_engines)), actor_is_impostor\n",
    "    )\n",
    "\n",
    "    return s_t, a_t, r_t, d_t, v_t, summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opponent_pool: list[player.Player] = [\n",
    "    player.RandomPlayer(),\n",
    "    player.GreedyPlayer(),\n",
    "]\n",
    "\n",
    "# a temp buffer of the current model's rewards\n",
    "crewmate_rewards_vs:defaultdict[tuple[str, str, str], list[float]] = defaultdict(lambda:[])\n",
    "impostor_rewards_vs:defaultdict[tuple[str, str, str], list[float]] = defaultdict(lambda:[])\n",
    "\n",
    "\n",
    "# the history of rewards over time\n",
    "crewmate_rewards_history:defaultdict[tuple[str, str, str], list[tuple[int, float]]] = defaultdict(lambda:[])\n",
    "impostor_rewards_history:defaultdict[tuple[str, str, str], list[tuple[int, float]]] = defaultdict(lambda:[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     others \u001b[39m=\u001b[39m [random\u001b[39m.\u001b[39mchoice(opponent_pool)]\u001b[39m*\u001b[39m\u001b[39m3\u001b[39m\n\u001b[1;32m     23\u001b[0m     \u001b[39m# play the game\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     future \u001b[39m=\u001b[39m play(nn_player, \u001b[39mFalse\u001b[39;49;00m, others)\n\u001b[1;32m     25\u001b[0m     futures\u001b[39m.\u001b[39mappend(future)\n\u001b[1;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPISODES_PER_AGENT):\n",
      "Cell \u001b[0;32mIn[3], line 69\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(actor_engine, actor_is_impostor, other_engines)\u001b[0m\n\u001b[1;32m     67\u001b[0m actions \u001b[39m=\u001b[39m {}\n\u001b[1;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m agent, agent_engine \u001b[39min\u001b[39;00m agent_engines\u001b[39m.\u001b[39mitems():\n\u001b[0;32m---> 69\u001b[0m     chosen_action \u001b[39m=\u001b[39m agent_engine\u001b[39m.\u001b[39;49mplay(agent \u001b[39m==\u001b[39;49m impostor, last_obs[agent])\n\u001b[1;32m     70\u001b[0m     actions[agent] \u001b[39m=\u001b[39m chosen_action\n\u001b[1;32m     71\u001b[0m     \u001b[39m# if the player is the actor we're gathering data for, then we need to store the data\u001b[39;00m\n",
      "File \u001b[0;32m~/myworkspace/omegasus/amogus-rl/player.py:47\u001b[0m, in \u001b[0;36mActorPlayer.play\u001b[0;34m(self, impostor, obs)\u001b[0m\n\u001b[1;32m     41\u001b[0m actor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimpostor_actor \u001b[39mif\u001b[39;00m impostor \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrewmate_actor\n\u001b[1;32m     43\u001b[0m device \u001b[39m=\u001b[39m network\u001b[39m.\u001b[39mdeviceof(actor)\n\u001b[1;32m     45\u001b[0m action_probs \u001b[39m=\u001b[39m (\n\u001b[1;32m     46\u001b[0m     actor\u001b[39m.\u001b[39;49mforward(network\u001b[39m.\u001b[39;49mobs_to_tensor(obs, device))[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m---> 47\u001b[0m     \u001b[39m.\u001b[39;49mto(\u001b[39m\"\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     48\u001b[0m     \u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     49\u001b[0m     \u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m chosen_action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mAction(\n\u001b[1;32m     53\u001b[0m     np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(env\u001b[39m.\u001b[39mACTION_SPACE_SIZE, p\u001b[39m=\u001b[39maction_probs)\n\u001b[1;32m     54\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m chosen_action\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for _ in range(TRAIN_EPOCHS):\n",
    "    crewmate_s_batch:list[env.Observation] = []\n",
    "    crewmate_a_batch:list[env.Action] = []\n",
    "    crewmate_r_batch:list[float] = []\n",
    "    crewmate_d_batch:list[float] = []\n",
    "    crewmate_v_batch:list[float] = []\n",
    "    \n",
    "    impostor_s_batch:list[env.Observation] = []\n",
    "    impostor_a_batch:list[env.Action] = []\n",
    "    impostor_r_batch:list[float] = []\n",
    "    impostor_d_batch:list[float] = []\n",
    "    impostor_v_batch:list[float] = []\n",
    "\n",
    "    # create actor player\n",
    "    nn_player = player.ActorPlayer(\n",
    "        impostor_actor, impostor_critic, impostor_step,\n",
    "        crewmate_actor, crewmate_critic, crewmate_step,\n",
    "    )\n",
    "\n",
    "    futures = []\n",
    "    for i in range(EPISODES_PER_AGENT):\n",
    "        others = [random.choice(opponent_pool)]*3\n",
    "        # play the game\n",
    "        future = play(nn_player, False, others)\n",
    "        futures.append(future)\n",
    "\n",
    "    for i in range(EPISODES_PER_AGENT):\n",
    "        others = [random.choice(opponent_pool)]*3\n",
    "        # play the game\n",
    "        future = play(nn_player, True, others)\n",
    "        futures.append(future)\n",
    "\n",
    "    \n",
    "    for future in futures:\n",
    "        s_t, a_t, r_t, d_t, v_t, game_summary = future\n",
    "\n",
    "        configuration = game_summary.configuration\n",
    "        r = np.sum(r_t)\n",
    "\n",
    "        if game_summary.actor_is_impostor:\n",
    "            impostor_s_batch += s_t\n",
    "            impostor_a_batch += a_t\n",
    "            impostor_r_batch += r_t\n",
    "            impostor_d_batch += d_t\n",
    "            impostor_v_batch += v_t\n",
    "            # statistics\n",
    "            impostor_rewards_vs[configuration] += [r]\n",
    "        else:\n",
    "            crewmate_s_batch += s_t\n",
    "            crewmate_a_batch += a_t\n",
    "            crewmate_r_batch += r_t\n",
    "            crewmate_d_batch += d_t\n",
    "            crewmate_v_batch += v_t\n",
    "            # statistics\n",
    "            crewmate_rewards_vs[configuration] += [r]\n",
    "\n",
    "\n",
    "    crewmate_actor_losses, crewmate_critic_losses = network.train_ppo(\n",
    "        crewmate_actor,\n",
    "        crewmate_critic,\n",
    "        crewmate_actor_optimizer,\n",
    "        crewmate_critic_optimizer,\n",
    "        crewmate_s_batch,\n",
    "        crewmate_a_batch,\n",
    "        crewmate_d_batch,\n",
    "        crewmate_v_batch\n",
    "    )\n",
    "\n",
    "    impostor_actor_losses, impostor_critic_losses = network.train_ppo(\n",
    "        impostor_actor,\n",
    "        impostor_critic,\n",
    "        impostor_actor_optimizer,\n",
    "        impostor_critic_optimizer,\n",
    "        impostor_s_batch,\n",
    "        impostor_a_batch,\n",
    "        impostor_d_batch,\n",
    "        impostor_v_batch\n",
    "    )\n",
    "\n",
    "    for crewmate_actor_loss, crewmate_critic_loss, impostor_actor_loss, impostor_critic_loss in zip(crewmate_actor_losses, crewmate_critic_losses, impostor_actor_losses, impostor_critic_losses):\n",
    "        writer.add_scalar('impostor_actor_loss', impostor_actor_loss, impostor_step)\n",
    "        writer.add_scalar('impostor_critic_loss', impostor_critic_loss, impostor_step)\n",
    "\n",
    "        writer.add_scalar('crewmate_actor_loss', crewmate_actor_loss, crewmate_step)\n",
    "        writer.add_scalar('crewmate_critic_loss', crewmate_critic_loss, crewmate_step)\n",
    "\n",
    "        if impostor_step % SUMMARY_STATS_INTERVAL == 0:\n",
    "            for opponent_name, rewards in impostor_rewards_vs.items():\n",
    "                if len(rewards) > 50:\n",
    "                    avg_reward = np.array(rewards).mean()\n",
    "                    writer.add_scalar(f'impostor_reward_against_{opponent_name}', avg_reward, impostor_step)\n",
    "                    impostor_rewards_vs[opponent_name] = []\n",
    "                    impostor_rewards_history[opponent_name] += [(impostor_step, avg_reward)]\n",
    "\n",
    "        if crewmate_step % SUMMARY_STATS_INTERVAL == 0:\n",
    "            for opponent_name, rewards in crewmate_rewards_vs.items():\n",
    "                if len(rewards) > 50:\n",
    "                    avg_reward = np.array(rewards).mean()\n",
    "                    writer.add_scalar(f'crewmate_reward_against_{opponent_name}', avg_reward, crewmate_step)\n",
    "                    crewmate_rewards_vs[opponent_name] = []\n",
    "                    crewmate_rewards_history[opponent_name] += [(crewmate_step, avg_reward)]\n",
    "\n",
    "        def clone_nn(nn):\n",
    "            new_nn = copy.deepcopy(nn)\n",
    "            new_nn.eval()\n",
    "            new_nn.to(device)\n",
    "            return new_nn\n",
    "\n",
    "        if impostor_step % MAKE_OPPONENT_INTERVAL == 0:\n",
    "            # create a new opponent\n",
    "            frozen_impostor_actor = clone_nn(impostor_actor)\n",
    "            frozen_impostor_critic = clone_nn(impostor_critic)\n",
    "            frozen_crewmate_actor = clone_nn(crewmate_actor)\n",
    "            frozen_crewmate_critic = clone_nn(crewmate_critic)\n",
    "            frozen_nn_player = player.ActorPlayer(\n",
    "                frozen_impostor_actor, frozen_impostor_critic, impostor_step,\n",
    "                frozen_crewmate_actor, frozen_crewmate_critic, crewmate_step,\n",
    "            )\n",
    "            opponent_pool.append(frozen_nn_player)\n",
    "\n",
    "        # Save the neural net parameters to disk.\n",
    "        if impostor_step % MODEL_SAVE_INTERVAL == 0:\n",
    "            torch.save(impostor_actor.state_dict(), f\"{SUMMARY_DIR}/impostor_model_ep_{impostor_step}_actor.ckpt\")\n",
    "            torch.save(impostor_critic.state_dict(), f\"{SUMMARY_DIR}/impostor_model_ep_{impostor_step}_critic.ckpt\")\n",
    "\n",
    "        # Save the neural net parameters to disk.\n",
    "        if crewmate_step % MODEL_SAVE_INTERVAL == 0:\n",
    "            torch.save(crewmate_actor.state_dict(), f\"{SUMMARY_DIR}/crewmate_model_ep_{crewmate_step}_actor.ckpt\")\n",
    "            torch.save(crewmate_critic.state_dict(), f\"{SUMMARY_DIR}/crewmate_model_ep_{crewmate_step}_critic.ckpt\")\n",
    "        \n",
    "        crewmate_step += 1\n",
    "        impostor_step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interrupted kernel due to convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others = []\n",
    "for _ in range(3):\n",
    "    others.append(player.RandomPlayer())\n",
    "# play the game            \n",
    "s_t, a_t, r_t, d_t, v_t, _ = play(nn_player, False, others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧑‍🚀⬛📦🧱🧱\n",
      "📦⬛👽🧱🧱\n",
      "⬛⬛📦🧱🧱\n",
      "📦📦⬛🧱🧱\n",
      "tensor([[0.1064, 0.0880, 0.2814, 0.3607, 0.1636]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Left\n",
      "reward: 0.0\n",
      "advantage: 0.04035360699999998\n",
      "value: 0.04035360699999998\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧑‍🚀⬛⬛📦🧱\n",
      "📦📦👽⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "⬛📦🧑‍🚀⬛🧱\n",
      "tensor([[0.4700, 0.0737, 0.1138, 0.1732, 0.1693]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Up\n",
      "reward: 0.0\n",
      "advantage: 0.05764800999999997\n",
      "value: 0.05764800999999997\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "🧑‍🚀⬛👽📦🧱\n",
      "📦📦⬛⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "tensor([[0.2464, 0.3238, 0.1513, 0.1380, 0.1405]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Left\n",
      "reward: 0.0\n",
      "advantage: 0.08235429999999996\n",
      "value: 0.08235429999999996\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛🧑‍🚀👽⬛📦\n",
      "⬛📦📦⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "tensor([[0.3485, 0.0810, 0.1513, 0.2585, 0.1608]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Wait\n",
      "reward: 0.0\n",
      "advantage: 0.11764899999999995\n",
      "value: 0.11764899999999995\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛🧑‍🚀👽⬛📦\n",
      "⬛📦📦⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "tensor([[0.1513, 0.1344, 0.1759, 0.3620, 0.1764]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Right\n",
      "reward: 0.0\n",
      "advantage: 0.16806999999999994\n",
      "value: 0.16806999999999994\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽📦🧱\n",
      "🧑‍🚀📦⬛⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "tensor([[0.1607, 0.3737, 0.1689, 0.1418, 0.1548]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Down\n",
      "reward: 0.0\n",
      "advantage: 0.24009999999999992\n",
      "value: 0.24009999999999992\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛⬛📦🧱\n",
      "🧑‍🚀📦👽⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "⬛🧑‍🚀⬛⬛🧱\n",
      "tensor([[0.5736, 0.0864, 0.0806, 0.1241, 0.1352]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Up\n",
      "reward: 0.0\n",
      "advantage: 0.3429999999999999\n",
      "value: 0.3429999999999999\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽📦🧱\n",
      "🧑‍🚀📦⬛⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "tensor([[0.1607, 0.3737, 0.1689, 0.1418, 0.1548]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Left\n",
      "reward: 0.0\n",
      "advantage: 0.48999999999999994\n",
      "value: 0.48999999999999994\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽⬛📦\n",
      "⬛🧑‍🚀📦⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "tensor([[0.1199, 0.1219, 0.1439, 0.4591, 0.1552]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Up\n",
      "reward: 0.0\n",
      "advantage: 0.7\n",
      "value: 0.7\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽⬛📦\n",
      "⬛⬛🧑‍🚀⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "tensor([[0.0783, 0.1340, 0.1294, 0.5012, 0.1571]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Down\n",
      "reward: 1.0\n",
      "advantage: 1.0\n",
      "value: 1.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛⬛⬛📦\n",
      "⬛⬛👽⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "⬛⬛⬛⬛⬛\n",
      "tensor([[0.1102, 0.1817, 0.1257, 0.1357, 0.4467]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Up\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽⬛📦\n",
      "⬛⬛💀⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "tensor([[0.0968, 0.1617, 0.1736, 0.4088, 0.1591]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Up\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽⬛📦\n",
      "⬛⬛💀⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "tensor([[0.0968, 0.1617, 0.1736, 0.4088, 0.1591]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Down\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛⬛⬛📦\n",
      "⬛⬛💀⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "⬛⬛⬛⬛⬛\n",
      "tensor([[0.1059, 0.1797, 0.1203, 0.1334, 0.4608]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Wait\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛⬛⬛📦\n",
      "⬛⬛💀⬛⬛\n",
      "⬛⬛⬛⬛📦\n",
      "⬛⬛⬛⬛⬛\n",
      "tensor([[0.1059, 0.1797, 0.1203, 0.1334, 0.4608]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Right\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛⬛📦🧱\n",
      "⬛💀👽⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "⬛⬛⬛⬛🧱\n",
      "tensor([[0.3884, 0.1484, 0.1426, 0.1351, 0.1856]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Wait\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛⬛📦🧱\n",
      "⬛💀👽⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "⬛⬛⬛⬛🧱\n",
      "tensor([[0.3884, 0.1484, 0.1426, 0.1351, 0.1856]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Up\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽📦🧱\n",
      "⬛💀⬛⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "tensor([[0.1338, 0.4044, 0.1578, 0.1498, 0.1542]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Up\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽📦🧱\n",
      "⬛💀⬛⬛🧱\n",
      "⬛⬛⬛📦🧱\n",
      "tensor([[0.1338, 0.4044, 0.1578, 0.1498, 0.1542]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Right\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽🧱🧱\n",
      "💀⬛⬛🧱🧱\n",
      "⬛⬛📦🧱🧱\n",
      "tensor([[0.1171, 0.1765, 0.1973, 0.1786, 0.3305]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Right\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽🧱🧱\n",
      "💀⬛⬛🧱🧱\n",
      "⬛⬛📦🧱🧱\n",
      "tensor([[0.1171, 0.1765, 0.1973, 0.1786, 0.3305]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Up\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽🧱🧱\n",
      "💀⬛⬛🧱🧱\n",
      "⬛⬛📦🧱🧱\n",
      "tensor([[0.1171, 0.1765, 0.1973, 0.1786, 0.3305]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Wait\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽🧱🧱\n",
      "💀⬛⬛🧱🧱\n",
      "⬛⬛📦🧱🧱\n",
      "tensor([[0.1171, 0.1765, 0.1973, 0.1786, 0.3305]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Move Right\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽🧱🧱\n",
      "💀⬛⬛🧱🧱\n",
      "⬛⬛📦🧱🧱\n",
      "tensor([[0.1171, 0.1765, 0.1973, 0.1786, 0.3305]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Wait\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n",
      "-----------------------------------\n",
      "🧱🧱🧱🧱🧱\n",
      "🧱🧱🧱🧱🧱\n",
      "⬛⬛👽🧱🧱\n",
      "💀⬛⬛🧱🧱\n",
      "⬛⬛📦🧱🧱\n",
      "tensor([[0.1171, 0.1765, 0.1973, 0.1786, 0.3305]], device='cuda:0',\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "Wait\n",
      "reward: 0.0\n",
      "advantage: 0.0\n",
      "value: 0.0\n"
     ]
    }
   ],
   "source": [
    "for s, a, r, d, v in zip(s_t, a_t, r_t, d_t, v_t):\n",
    "    print(\"-----------------------------------\")\n",
    "    env.print_obs(s)\n",
    "    print(crewmate_actor.forward(network.obs_to_tensor(s, device)))\n",
    "    env.print_action(a)\n",
    "    print(\"reward:\", r)\n",
    "    print(\"advantage:\", d)\n",
    "    print(\"value:\", v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "",
       "name": "",
       "stack": "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n\n\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the crewmate rewards against random opponents\n",
    "rx, ry= zip(*crewmate_rewards_history[('random', 'random', 'random')])\n",
    "gx, gy= zip(*crewmate_rewards_history[('greedy', 'greedy', 'greedy')])\n",
    "rx = [x / 10 for x in rx]\n",
    "gx = [x / 10 for x in gx]\n",
    "\n",
    "\n",
    "plt.plot(rx, ry, label='Reward against Random Impostor')\n",
    "plt.plot(gx, gy, label='Reward against Engineered Impostor')\n",
    "\n",
    "plt.title('(Crewmate) PPO Learned Policy vs Various Impostor Policies')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.code.notebook.error": {
       "message": "",
       "name": "",
       "stack": "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n\n\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot the impostor against random opponents\n",
    "rx, ry= zip(*impostor_rewards_history[('random', 'random', 'random')])\n",
    "gx, gy= zip(*impostor_rewards_history[('greedy', 'greedy', 'greedy')])\n",
    "rx = [x / 10 for x in rx]\n",
    "gx = [x / 10 for x in gx]\n",
    "\n",
    "plt.plot(rx, ry, label='Reward against Random Crewmate')\n",
    "plt.plot(gx, gy, label='Reward against Engineered Crewmate')\n",
    "plt.title('(Impostor) PPO Learned Policy vs Various Crewmate Policies')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "def play_benchmark(engine: player.Player, is_impostor: bool, other_engines: list[player.Player]) -> list[env.Reward]:\n",
    "    # create environment\n",
    "    initial_state = env.State(\n",
    "        [],\n",
    "        np.zeros((BOARD_XSIZE, BOARD_YSIZE), dtype=np.int8),\n",
    "    )\n",
    "\n",
    "    # randomize task location\n",
    "    for _ in range(5):\n",
    "        location = random_valid_location()\n",
    "        initial_state.tasks[location] += 5\n",
    "\n",
    "    # create actor player at random location\n",
    "    actor_playerstate = env.PlayerState(random_valid_location(), is_impostor, False)\n",
    "    # create other players at random locations\n",
    "    other_playerstate = [env.PlayerState(random_valid_location(), False, False) for _ in other_engines]\n",
    "    # If the actor is not an impostor, then the impostor is randomly chosen from the others.\n",
    "    if not is_impostor:\n",
    "        random.choice(other_playerstate).impostor = True\n",
    "\n",
    "    # set the players in the environment\n",
    "    initial_state.players = [actor_playerstate] + other_playerstate\n",
    "    # set the player engines\n",
    "    player_engines = [engine] + other_engines\n",
    "\n",
    "    # shuffle the player indices such that the corresponding player states and engines have the same indices\n",
    "    random_indices = np.random.permutation(len(player_engines))\n",
    "    initial_state.players = [initial_state.players[i] for i in random_indices]\n",
    "    player_engines = [player_engines[i] for i in random_indices]\n",
    "\n",
    "    actor_index = env.Player(np.argwhere(random_indices==0)[0][0])\n",
    "\n",
    "    e = env.Env(initial_state)\n",
    "\n",
    "    r_t: list[env.Reward] = []\n",
    "    # play the game\n",
    "    while not e.game_over():\n",
    "        for player, player_engine in enumerate(player_engines):\n",
    "            player = env.Player(player)\n",
    "            if player == actor_index:\n",
    "                _, _, chosen_action = player_engine.play(player, e)\n",
    "                e.play(chosen_action, player)\n",
    "            else:\n",
    "                if e.game_over_for(player):\n",
    "                    continue\n",
    "                _, _, chosen_action = player_engine.play(player, e)\n",
    "                e.play(chosen_action, player)\n",
    "        # step and get rewards\n",
    "        rewards = e.step()\n",
    "        r_t += [rewards[actor_index]]\n",
    "        # if the actor we're gathering data for is dead, then we need to stop\n",
    "        if e.game_over_for(env.Player(actor_index)):\n",
    "            break\n",
    "\n",
    "    return r_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'prompt_toolkit.formatted_text'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "SAMPLES = 50\n",
    "\n",
    "nn_crewmate_vs_random_impostor = []\n",
    "random_crewmate_vs_random_impostor = []\n",
    "greedy_crewmate_vs_random_impostor = []\n",
    "\n",
    "# run a couple simulations to find how well the learned policy does against a random impostor\n",
    "for _ in range(SAMPLES):\n",
    "    r_t = play_benchmark(nn_player, False, [player.RandomPlayer()]*3)\n",
    "    nn_crewmate_vs_random_impostor.append(np.sum(r_t))\n",
    "\n",
    "# run a couple simulations to find how well the random policy does against a random impostor\n",
    "for _ in range(SAMPLES):\n",
    "    r_t = play_benchmark(player.RandomPlayer(), False, [player.RandomPlayer()]*3)\n",
    "    random_crewmate_vs_random_impostor.append(np.sum(r_t))\n",
    "\n",
    "# run a couple simulations to find how well the greedy policy does against a random impostor\n",
    "for _ in range(SAMPLES):\n",
    "    r_t = play_benchmark(player.GreedyPlayer(), False, [player.RandomPlayer()]*3)\n",
    "    greedy_crewmate_vs_random_impostor.append(np.sum(r_t))\n",
    "\n",
    "\n",
    "nn_impostor_vs_random_crewmate = []\n",
    "random_impostor_vs_random_crewmate = []\n",
    "greedy_impostor_vs_random_crewmate = []\n",
    "\n",
    "# run a couple simulations to find how well the learned policy does against a random crewmate\n",
    "for _ in range(SAMPLES):\n",
    "    r_t = play_benchmark(nn_player, True, [player.RandomPlayer()]*3)\n",
    "    nn_impostor_vs_random_crewmate.append(np.sum(r_t))\n",
    "\n",
    "# run a couple simulations to find how well the random policy does against a random crewmate\n",
    "for _ in range(SAMPLES):\n",
    "    r_t = play_benchmark(player.RandomPlayer(), True, [player.RandomPlayer()]*3)\n",
    "    random_impostor_vs_random_crewmate.append(np.sum(r_t))\n",
    "\n",
    "# run a couple simulations to find how well the greedy policy does against a random crewmate\n",
    "for _ in range(SAMPLES):\n",
    "    r_t = play_benchmark(player.GreedyPlayer(), True, [player.RandomPlayer()]*3)\n",
    "    greedy_impostor_vs_random_crewmate.append(np.sum(r_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"nn_crewmate_vs_random_impostor\", np.mean(nn_crewmate_vs_random_impostor))\n",
    "print(\"random_crewmate_vs_random_impostor\", np.mean(random_crewmate_vs_random_impostor))\n",
    "print(\"engineered_crewmate_vs_random_impostor\", np.mean(greedy_crewmate_vs_random_impostor))\n",
    "print()\n",
    "print(\"nn_impostor_vs_random_crewmate\", np.mean(nn_impostor_vs_random_crewmate))\n",
    "print(\"random_impostor_vs_random_crewmate\", np.mean(random_impostor_vs_random_crewmate))\n",
    "print(\"engineered_impostor_vs_random_crewmate\", np.mean(greedy_impostor_vs_random_crewmate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(\"nn_policy_vs_random_policy (crewmate):\" , np.mean(nn_crewmate_vs_random_impostor)/np.mean(random_crewmate_vs_random_impostor))\n",
    "print(\"nn_policy_vs_engineered_policy (crewmate):\" , np.mean(nn_crewmate_vs_random_impostor)/np.mean(greedy_crewmate_vs_random_impostor))\n",
    "print()\n",
    "print(\"nn_policy_vs_random_policy (impostor):\" , np.mean(nn_impostor_vs_random_crewmate)/np.mean(random_impostor_vs_random_crewmate))\n",
    "print(\"nn_policy_vs_engineered_policy (impostor):\" , np.mean(nn_impostor_vs_random_crewmate)/np.mean(greedy_impostor_vs_random_crewmate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
